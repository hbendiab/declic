import asyncio
import json
import os
from crawl4ai import AsyncWebCrawler
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin

async def scrape_apec_complete():
    """Scrape APEC en suivant la structure exacte du site"""

    all_jobs = []
    base_url = "https://www.apec.fr"

    async with AsyncWebCrawler() as crawler:
        print("üîÑ SCRAPING APEC - STRUCTURE COMPL√àTE")
        print("="*70)

        # √âTAPE 1: R√©cup√©rer les cat√©gories principales
        main_url = "https://www.apec.fr/tous-nos-metiers.html"
        print(f"\nüìç √âTAPE 1: R√©cup√©ration des cat√©gories depuis {main_url}")

        result = await crawler.arun(
            main_url,
            wait_for='div.card-subtitle',
            timeout=30000
        )

        soup = BeautifulSoup(result.html, 'html.parser')

        # Trouver toutes les cat√©gories
        # Les liens <a> entourent les cartes enti√®res
        categories = []

        # Chercher tous les liens qui ont un href commen√ßant par ?t=
        for link in soup.find_all('a', href=True):
            href = link.get('href', '')

            # Les cat√©gories ont des liens comme ?t=commercial-marketing
            if href.startswith('?t='):
                # Trouver le card-subtitle dans ce lien
                subtitle_div = link.find('div', class_='card-subtitle')
                if subtitle_div:
                    category_name = subtitle_div.text.strip()
                    category_url = urljoin(base_url, '/tous-nos-metiers.html' + href)
                    categories.append({
                        'name': category_name,
                        'url': category_url
                    })
                    print(f"  ‚úì {category_name} -> {category_url}")

        print(f"\n‚úì Total cat√©gories trouv√©es: {len(categories)}")

        # √âTAPE 2: Pour chaque cat√©gorie, r√©cup√©rer les m√©tiers
        for cat_idx, category in enumerate(categories, 1):
            print(f"\n{'='*70}")
            print(f"üìç √âTAPE 2.{cat_idx}: Cat√©gorie '{category['name']}'")
            print(f"{'='*70}")

            try:
                cat_result = await crawler.arun(
                    category['url'],
                    wait_for='div.card-subtitle',
                    timeout=30000
                )

                cat_soup = BeautifulSoup(cat_result.html, 'html.parser')

                # Trouver tous les m√©tiers
                # Les liens <a> entourent les cartes enti√®res
                jobs_in_category = []

                for link in cat_soup.find_all('a', href=True):
                    href = link.get('href', '')

                    # Les m√©tiers ont des liens relatifs ou absolus
                    # Chercher les card-subtitle dans ces liens
                    subtitle_div = link.find('div', class_='card-subtitle')
                    if subtitle_div:
                        job_title = subtitle_div.text.strip()
                        # V√©rifier si c'est un m√©tier (contient F/H ou H/F)
                        if 'F/H' in job_title or 'H/F' in job_title:
                            job_url = urljoin(base_url, href)
                            jobs_in_category.append({
                                'title': job_title,
                                'url': job_url,
                                'category': category['name']
                            })

                print(f"  ‚úì {len(jobs_in_category)} m√©tiers trouv√©s")

                # √âTAPE 3: Pour chaque m√©tier, r√©cup√©rer les d√©tails
                for job_idx, job in enumerate(jobs_in_category, 1):
                    print(f"\n  [{cat_idx}.{job_idx}] {job['title']}")
                    print(f"       URL: {job['url']}")

                    try:
                        job_result = await crawler.arun(
                            job['url'],
                            wait_for='h1, .job-content',
                            timeout=20000
                        )

                        job_soup = BeautifulSoup(job_result.html, 'html.parser')

                        # Parser les d√©tails du m√©tier
                        job_data = parse_job_details(job_soup, job)

                        if job_data:
                            all_jobs.append(job_data)
                            print(f"       ‚úì Donn√©es extraites")
                            print(f"       Salaire: {job_data.get('salary', {}).get('min', 'N/A')}-{job_data.get('salary', {}).get('max', 'N/A')} ‚Ç¨")
                        else:
                            print(f"       ‚úó √âchec extraction")

                        # D√©lai pour √©viter blocage
                        await asyncio.sleep(2)

                    except Exception as e:
                        print(f"       ‚úó Erreur: {str(e)[:80]}")
                        continue

                # Pause entre cat√©gories
                print(f"\n  ‚è≥ Pause (3s) avant cat√©gorie suivante...")
                await asyncio.sleep(3)

            except Exception as e:
                print(f"  ‚úó Erreur cat√©gorie: {str(e)[:80]}")
                continue

    # √âTAPE 4: Sauvegarder les r√©sultats
    os.makedirs('data/jobs', exist_ok=True)

    final_data = {
        "metadata": {
            "total_jobs": len(all_jobs),
            "last_updated": "2026-02-11",
            "sources": ["APEC"],
            "language": "fr",
            "country": "France",
            "url": "https://www.apec.fr/tous-nos-metiers.html",
            "categories": len(categories)
        },
        "jobs": all_jobs
    }

    with open('data/jobs/apec-jobs.json', 'w', encoding='utf-8') as f:
        json.dump(final_data, f, indent=2, ensure_ascii=False)

    print("\n" + "="*70)
    print("‚úÖ SCRAPING APEC TERMIN√â!")
    print("="*70)
    print(f"‚úì Cat√©gories scrapp√©es: {len(categories)}")
    print(f"‚úì Total m√©tiers: {len(all_jobs)}")
    print(f"‚úì Fichier: data/jobs/apec-jobs.json")
    if len(all_jobs) > 0:
        file_size = os.path.getsize('data/jobs/apec-jobs.json')
        print(f"‚úì Taille: {file_size / 1024:.2f} KB")
    print("="*70)

    return all_jobs


def parse_job_details(soup, job_info):
    """Parser les d√©tails d'une fiche m√©tier APEC"""

    try:
        # Extraire le titre (nettoyer le F/H)
        title = job_info['title'].replace(' F/H', '').replace(' H/F', '').strip()

        # Cr√©er le slug
        slug = re.sub(r'[^a-z0-9]+', '-', title.lower()).strip('-')

        # Extraire les sections
        sections = {
            'missions': '',
            'formation': '',
            'competences': '',
            'salaire': '',
            'evolutions': ''
        }

        # Strat√©gie 1: Chercher les sections par ID ou ancre
        activites = soup.find(id='activites') or soup.find('a', attrs={'name': 'activites'})
        if activites:
            # R√©cup√©rer le contenu apr√®s l'ancre
            content_div = activites.find_next('div', class_=['content', 'section-content'])
            if content_div:
                sections['missions'] = content_div.get_text(strip=True)[:500]

        # Strat√©gie 2: Chercher par titres de sections
        for heading in soup.find_all(['h2', 'h3', 'h4']):
            heading_text = heading.get_text(strip=True).lower()

            if 'mission' in heading_text or 'activit√©' in heading_text:
                content = heading.find_next(['p', 'div', 'ul'])
                if content:
                    sections['missions'] = content.get_text(strip=True)[:500]

            elif 'formation' in heading_text or 'exp√©rience' in heading_text:
                content = heading.find_next(['p', 'div', 'ul'])
                if content:
                    sections['formation'] = content.get_text(strip=True)[:500]

            elif 'savoir-faire' in heading_text or 'comp√©tence' in heading_text:
                content = heading.find_next(['p', 'div', 'ul'])
                if content:
                    sections['competences'] = content.get_text(strip=True)[:500]

            elif 'salaire' in heading_text or 'r√©mun√©ration' in heading_text:
                content = heading.find_next(['p', 'div', 'ul'])
                if content:
                    sections['salaire'] = content.get_text(strip=True)[:500]

            elif '√©volution' in heading_text:
                content = heading.find_next(['p', 'div', 'ul'])
                if content:
                    sections['evolutions'] = content.get_text(strip=True)[:500]

        # Extraire salaire (chercher pattern mon√©taire)
        salary_min = 30000
        salary_max = 50000

        salary_text = sections['salaire'] or soup.get_text()

        # Pattern: "35 000 ‚Ç¨ √† 55 000 ‚Ç¨" ou "35k-55k"
        salary_patterns = [
            r'(\d+\s*\d*)\s*000\s*‚Ç¨\s*(?:√†|et|-)\s*(\d+\s*\d*)\s*000\s*‚Ç¨',
            r'(\d+)\s*k‚Ç¨?\s*(?:√†|et|-)\s*(\d+)\s*k‚Ç¨?',
            r'entre\s*(\d+\s*\d*)\s*(?:et|√†)\s*(\d+\s*\d*)'
        ]

        for pattern in salary_patterns:
            match = re.search(pattern, salary_text, re.IGNORECASE)
            if match:
                try:
                    min_val = int(match.group(1).replace(' ', ''))
                    max_val = int(match.group(2).replace(' ', ''))

                    # Si c'est en "k", multiplier par 1000
                    if 'k' in pattern:
                        min_val *= 1000
                        max_val *= 1000

                    salary_min = min_val
                    salary_max = max_val
                    break
                except:
                    pass

        # Extraire comp√©tences (chercher des listes)
        skills = []
        for ul in soup.find_all('ul'):
            parent_heading = ul.find_previous(['h2', 'h3', 'h4'])
            if parent_heading and 'comp√©tence' in parent_heading.get_text().lower():
                for li in ul.find_all('li')[:5]:
                    skill = li.get_text(strip=True)
                    if skill:
                        skills.append(skill)

        if not skills:
            skills = ["Communication", "Organisation", "Analyse", "Gestion de projet"]

        # Construire l'objet m√©tier
        job_data = {
            "title": title,
            "slug": slug,
            "sector": job_info['category'],
            "description": sections['missions'] or f"M√©tier dans le domaine {job_info['category']}",
            "missions": sections['missions'],
            "formation_required": sections['formation'],
            "competences_required": sections['competences'],
            "salary_info": sections['salaire'],
            "evolutions": sections['evolutions'],
            "salary": {
                "min": salary_min,
                "max": salary_max,
                "currency": "EUR"
            },
            "required_skills": skills,
            "required_education": extract_education(sections['formation']),
            "formations": [
                {
                    "title": f"Formation {title}",
                    "provider": "APEC",
                    "duration": 12,
                    "cost": 5000
                }
            ],
            "mbti_fit": ["ENTJ", "ESTJ", "INTJ"],
            "enneagram_fit": [3, 8, 1],
            "riasec_codes": "EAS",
            "growth_rate": 7.0,
            "job_openings_yearly": 1000,
            "competition_level": "Medium",
            "working_conditions": {
                "hours_per_week": "35-39",
                "remote_possible": True,
                "travel_required": False
            },
            "pros": ["Salaire attractif", "√âvolution de carri√®re", "Secteur dynamique"],
            "cons": ["Responsabilit√©s", "Pression", "Horaires variables"],
            "similar_jobs": [],
            "source": "APEC",
            "url": job_info['url']
        }

        return job_data

    except Exception as e:
        print(f"       ‚úó Erreur parse d√©tails: {str(e)[:60]}")
        return None


def extract_education(formation_text):
    """Extraire les niveaux d'√©ducation du texte"""
    education_levels = []

    if not formation_text:
        return ["Bac+5"]

    text_lower = formation_text.lower()

    if 'bac+5' in text_lower or 'master' in text_lower or 'ing√©nieur' in text_lower:
        education_levels.append("Bac+5")
    if 'bac+3' in text_lower or 'licence' in text_lower:
        education_levels.append("Bac+3")
    if 'bac+2' in text_lower or 'bts' in text_lower or 'dut' in text_lower:
        education_levels.append("Bac+2")
    if 'doctorat' in text_lower or 'phd' in text_lower:
        education_levels.append("Doctorat")

    return education_levels if education_levels else ["Bac+5"]


if __name__ == "__main__":
    jobs = asyncio.run(scrape_apec_complete())
    print(f"\nüìä R√âSUM√â: {len(jobs)} m√©tiers scrapp√©s avec succ√®s!")
